{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koralpc/image-captioning/blob/main/Image_Captioning_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Image Captioning\n",
        "\n",
        "This notebook provides a short tutorial on image captioning task. In this tutorial we will train a Encoder-Decoder network with attention and use ImageNet backbone for encoding. We will train the network on COCO2014 dataset using the images and the caption annotations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ja1zDrxBah"
      },
      "source": [
        "## Step 1: Copying the source\n",
        "The source code for the dataset, model and training is in my repo [image_captioning](https://github.com/koralpc/image-captioning). So we will first copy it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVYX9S0sxVpD"
      },
      "source": [
        "!git clone https://github.com/koralpc/image-captioning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uk5pQ2V9aVY"
      },
      "source": [
        "import os\n",
        "os.chdir('image-captioning')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63HvS2xyI7h"
      },
      "source": [
        "## Step 2: Download & setup dataset\n",
        "First we download the dataset. The dataset is around 13GB, so it might take a while to download and make sure you have enough space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_30UT3L-61"
      },
      "source": [
        "#@title Set your input variables\n",
        "#@markdown You can modify the fields here to change the dataset settings\n",
        "#@markdown limit_size limits how many instances you will use in the training dataset\n",
        "#@markdown top_k will keep top_k words in vocabulary\n",
        "\n",
        "annotation_url = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"  #@param\n",
        "img_url = \"http://images.cocodataset.org/zips/train2014.zip\"  #@param {type: \"string\"}\n",
        "buffer_size = 100  #@param {type: \"slider\", min: 1, max: 1000}\n",
        "limit_size = 100 #@param {type: \"slider\", min: 10, max: 10000}\n",
        "batch_size = 32  #@param {type: \"integer\"}\n",
        "top_k = 5000  #@param {type: \"integer\"}\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMoxI36aMxa7"
      },
      "source": [
        "### Initialize dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6AXc0jiyFxz"
      },
      "source": [
        "from src.dataset import ImageCaptionDataset\n",
        "caption_dataset = ImageCaptionDataset(img_url, annotation_url)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nQvI8HNAwo"
      },
      "source": [
        "First we fetch and extract the dataset. This step downloads the 13GB data and extracts the data. This step is both disk and RAM intensive so might take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N08vABVso5l"
      },
      "source": [
        "annotation_file, image_path = caption_dataset._fetch_dataset()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VTGrrpwNZxE"
      },
      "source": [
        "Now that dataset is downloaded, we will load the annotation files to extract the image paths and the captions per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7s6FOWXsuKm"
      },
      "source": [
        "train_captions, img_name_vector = caption_dataset.load_dataset(\n",
        "            annotation_file, image_path, limit_size=limit_size\n",
        "        )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Hy7_ZMN18c"
      },
      "source": [
        "Next step is for users with limited RAM. The image shapes are 8*8*2048 which can overflow the RAM during training. So if you have limited RAM space, you can run the code below, which will use pre-trained ImageNet to preprocess the images till the last layer before it's output layer. Then we will save these pre-processed features and train the network over it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4fbyEvVtHJi"
      },
      "source": [
        "caption_dataset.preprocess_features(img_name_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXHzs2MTOQpV"
      },
      "source": [
        "Next step after processing images is to tokenize the captions. Since we will use a RNN based decoder, our outputs will be caption vectors that are encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM9ekZJttZZ2"
      },
      "source": [
        "from src.preprocess import Preprocess\n",
        "cap_vector, max_length, tokenizer = Preprocess.tokenize(train_captions,top_k)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfLcdSJOZq4"
      },
      "source": [
        "After both images and captions are processed, we split the dataset into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YSArqetfRQ"
      },
      "source": [
        "img_name_train, cap_train, img_name_val, cap_val = caption_dataset.split_dataset(\n",
        "    img_name_vector, cap_vector\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-0werQrOegx"
      },
      "source": [
        "Finally, we construct a `tf.data.Dataset` element for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwqivVQt8A1"
      },
      "source": [
        "train_dataset = caption_dataset.create_dataset(\n",
        "    img_name_train, cap_train, buffer_size, batch_size\n",
        ")\n",
        "val_dataset = caption_dataset.create_dataset(\n",
        "    img_name_val, cap_val, buffer_size, batch_size\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoITRgqOpFL"
      },
      "source": [
        "### Note\n",
        "If you want do all these steps at once in your code, just uncomment the one-liner version below, which will give you the same output as the steps above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDHfvnHRwXs-"
      },
      "source": [
        "#train_data, val_data, max_length, tokenizer = caption_dataset.prepare_data(limit_size, buffer_size, batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZqR5wjO1sN"
      },
      "source": [
        "## Setting up the model\n",
        "In this tutorial we use a Encoder-Decoder Model with attention\n",
        "You can play with the variables below to find the optimal setting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X6g48xrL-nk"
      },
      "source": [
        "#@title Set your model parameters\n",
        "#@markdown Here you can play with some of the variables used in model architecture\n",
        "embedding_dim = 512  #@param {type: \"integer\"}\n",
        "units = 1024  #@param {type: \"integer\"}\n",
        "vocab_size = top_k + 1\n",
        "num_epochs = 2  #@param {type: \"integer\"}\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO94p64QbNQR",
        "outputId": "ac83d5b8-3738-40d2-c1eb-eb811655da42"
      },
      "source": [
        "!rm -r checkpoints"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'checkpoints': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVQQXACP61Q"
      },
      "source": [
        "import tensorflow as tf\n",
        "from src.model import CNN_Encoder,RNN_Decoder\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim,units,vocab_size)\n",
        "optimizer  = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3lRDNAzPC4M"
      },
      "source": [
        "## Setting up the trainer\n",
        "In the tutorial we will use a separate trainer class, which manages the training/evaluation,loading and saving of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyrLDmGQNmE7"
      },
      "source": [
        "from src.train import Trainer\n",
        "checkpoint_dir = \"./checkpoints/train\"\n",
        "train_config = dict(buffer_size=buffer_size,limit_size=limit_size,batch_size=batch_size,max_length=max_length,attn_shape=attention_features_shape)\n",
        "trainer = Trainer(checkpoint_path=checkpoint_dir,train_config=train_config,encoder=encoder,decoder=decoder,optimizer=optimizer,tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE5aTIePPLe6"
      },
      "source": [
        "Here we set the checkpoint directory and initialize the manager for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Pv7B9UN3wG"
      },
      "source": [
        "num_steps = len(train_dataset) // batch_size\n",
        "trainer.set_checkpoint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH9Jra3pPQDl"
      },
      "source": [
        "#### Start of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSzPGsuDN47a"
      },
      "source": [
        "trainer.train(train_dataset,num_epochs,num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qqMslsPSDG"
      },
      "source": [
        "## Evaluation\n",
        "For evaluation, we use the `eval_single` function of the trainer object, which takes the validation `tf.data.Dataset` we have prepared before. This function randomly selects an image from the dataset and predicts the caption for it. The captions are then displayed with the attention plots per word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1yEKpNcrxDe"
      },
      "source": [
        "trainer.eval_single(model,cap_val,img_name_val,visualise=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8gDcyXP_JO"
      },
      "source": [
        "## Exporting the model\n",
        "Next step is to export our model.\n",
        "We will export in `tf.SavedModel`, so it is possible to reload the model on TF.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QezAkxDNONAa"
      },
      "source": [
        "Our model has 17M parameters. Now we will save it in tf SavedModel format. Since our overall model consists of the encoder and decoder, we will save these separately and then recover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpw8y3gCPICD"
      },
      "source": [
        "from src.model import ImageCaptioner\n",
        "from src.preprocess import image_features_extract_model\n",
        "filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~'\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "            num_words=top_k, oov_token=\"<unk>\", filters=filters\n",
        "        )\n",
        "tokenizer.fit_on_texts([[\"<start>\",\"<end>\"]])\n",
        "tokenizer.word_index[\"<pad>\"] = 0\n",
        "tokenizer.index_word[0] = \"<pad>\"\n",
        "image_captioner = ImageCaptioner(encoder = encoder, decoder = decoder, tokenizer=tokenizer)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQNiHALvr3He"
      },
      "source": [
        "image_captioner.caption(item[0],max_length,attention_features_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TPg0xVU6yNU"
      },
      "source": [
        "ic = ImageCaptioner2(encoder = encoder, decoder = decoder, tokenizer=tokenizer, image_features_extracter=image_features_extract_model)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tkvnI4vysML"
      },
      "source": [
        "tf.saved_model.save(ic, 'image_captioner',\n",
        "                    signatures={'serving_default': ic.caption})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvGgmPG5OZKQ"
      },
      "source": [
        "image_captioner_export = tf.saved_model.load(\"image_captioner\")"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o4dUn0Y8sJs"
      },
      "source": [
        "image_captioner_export.signatures[\"serving_default\"](img_tensor=tf.zeros((1,300,300,3),tf.dtypes.int32))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}