{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koralpc/image-captioning/blob/main/Image_Captioning_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Image Captioning\n",
        "\n",
        "This notebook provides a short tutorial on image captioning task. In this tutorial we will train a Encoder-Decoder network with attention and use ImageNet backbone for encoding. We will train the network on COCO2014 dataset using the images and the caption annotations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ja1zDrxBah"
      },
      "source": [
        "## Step 1: Copying the source\n",
        "The source code for the dataset, model and training is in my repo [image_captioning](https://github.com/koralpc/image-captioning). So we will first copy it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVYX9S0sxVpD"
      },
      "source": [
        "!git clone https://github.com/koralpc/image-captioning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uk5pQ2V9aVY"
      },
      "source": [
        "import os\n",
        "os.chdir('image-captioning')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63HvS2xyI7h"
      },
      "source": [
        "## Step 2: Download & setup dataset\n",
        "First we download the dataset. The dataset is around 13GB, so it might take a while to download and make sure you have enough space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_30UT3L-61"
      },
      "source": [
        "#@title Set your input variables\n",
        "#@markdown You can modify the fields here to change the dataset settings\n",
        "#@markdown limit_size limits how many instances you will use in the training dataset\n",
        "#@markdown top_k will keep top_k words in vocabulary\n",
        "\n",
        "annotation_url = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"  #@param\n",
        "img_url = \"http://images.cocodataset.org/zips/train2014.zip\"  #@param {type: \"string\"}\n",
        "buffer_size = 100  #@param {type: \"slider\", min: 1, max: 1000}\n",
        "limit_size = 10000 #@param {type: \"slider\", min: 10, max: 10000}\n",
        "batch_size = 64  #@param {type: \"integer\"}\n",
        "top_k = 5000  #@param {type: \"integer\"}\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMoxI36aMxa7"
      },
      "source": [
        "### Initialize dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6AXc0jiyFxz"
      },
      "source": [
        "from src.dataset import ImageCaptionDataset\n",
        "caption_dataset = ImageCaptionDataset(img_url, annotation_url)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nQvI8HNAwo"
      },
      "source": [
        "First we fetch and extract the dataset. This step downloads the 13GB data and extracts the data. This step is both disk and RAM intensive so might take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N08vABVso5l"
      },
      "source": [
        "annotation_file, image_path = caption_dataset._fetch_dataset()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VTGrrpwNZxE"
      },
      "source": [
        "Now that dataset is downloaded, we will load the annotation files to extract the image paths and the captions per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7s6FOWXsuKm"
      },
      "source": [
        "train_captions, img_name_vector = caption_dataset.load_dataset(\n",
        "            annotation_file, image_path, limit_size=limit_size\n",
        "        )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Hy7_ZMN18c"
      },
      "source": [
        "Next step is for users with limited RAM. The image shapes are 8*8*2048 which can overflow the RAM during training. So if you have limited RAM space, you can run the code below, which will use pre-trained ImageNet to preprocess the images till the last layer before it's output layer. Then we will save these pre-processed features and train the network over it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4fbyEvVtHJi"
      },
      "source": [
        "caption_dataset.preprocess_features(img_name_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXHzs2MTOQpV"
      },
      "source": [
        "Next step after processing images is to tokenize the captions. Since we will use a RNN based decoder, our outputs will be caption vectors that are encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM9ekZJttZZ2"
      },
      "source": [
        "from src.preprocess import Preprocess\n",
        "cap_vector, max_length, tokenizer = Preprocess.tokenize(train_captions,top_k)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfLcdSJOZq4"
      },
      "source": [
        "After both images and captions are processed, we split the dataset into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YSArqetfRQ"
      },
      "source": [
        "img_name_train, cap_train, img_name_val, cap_val = caption_dataset.split_dataset(\n",
        "    img_name_vector, cap_vector\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-0werQrOegx"
      },
      "source": [
        "Finally, we construct a `tf.data.Dataset` element for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwqivVQt8A1"
      },
      "source": [
        "train_dataset = caption_dataset.create_dataset(\n",
        "    img_name_train, cap_train, buffer_size, batch_size\n",
        ")\n",
        "val_dataset = caption_dataset.create_dataset(\n",
        "    img_name_val, cap_val, buffer_size, batch_size\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoITRgqOpFL"
      },
      "source": [
        "### Note\n",
        "If you want do all these steps at once in your code, just uncomment the one-liner version below, which will give you the same output as the steps above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDHfvnHRwXs-"
      },
      "source": [
        "#train_data, val_data, max_length, tokenizer = caption_dataset.prepare_data(limit_size, buffer_size, batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZqR5wjO1sN"
      },
      "source": [
        "## Setting up the model\n",
        "In this tutorial we use a Encoder-Decoder Model with attention\n",
        "You can play with the variables below to find the optimal setting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X6g48xrL-nk"
      },
      "source": [
        "#@title Set your model parameters\n",
        "#@markdown Here you can play with some of the variables used in model architecture\n",
        "embedding_dim = 512  #@param {type: \"integer\"}\n",
        "units = 1024  #@param {type: \"integer\"}\n",
        "vocab_size = top_k + 1\n",
        "num_epochs = 40  #@param {type: \"integer\"}\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVQQXACP61Q"
      },
      "source": [
        "from src.model import EDModel\n",
        "model = EDModel(embedding_dim,units,vocab_size,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3lRDNAzPC4M"
      },
      "source": [
        "## Setting up the trainer\n",
        "In the tutorial we will use a separate trainer class, which manages the training/evaluation,loading and saving of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyrLDmGQNmE7"
      },
      "source": [
        "from src.train import Trainer\n",
        "checkpoint_dir = \"./checkpoints/train\"\n",
        "train_config = dict(buffer_size=buffer_size,limit_size=limit_size,batch_size=batch_size,max_length=max_length,attn_shape=attention_features_shape)\n",
        "trainer = Trainer(checkpoint_path=checkpoint_dir,train_config=train_config)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE5aTIePPLe6"
      },
      "source": [
        "Here we set the checkpoint directory and initialize the manager for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Pv7B9UN3wG"
      },
      "source": [
        "num_steps = len(train_dataset) // batch_size\n",
        "trainer.set_checkpoint(model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH9Jra3pPQDl"
      },
      "source": [
        "#### Start of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSzPGsuDN47a"
      },
      "source": [
        "trainer.train(model,train_dataset,num_epochs,num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qqMslsPSDG"
      },
      "source": [
        "## Evaluation\n",
        "For evaluation, we use the `eval_single` function of the trainer object, which takes the validation `tf.data.Dataset` we have prepared before. This function randomly selects an image from the dataset and predicts the caption for it. The captions are then displayed with the attention plots per word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1yEKpNcrxDe"
      },
      "source": [
        "trainer.eval_single(model,cap_val,img_name_val,visualise=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8gDcyXP_JO"
      },
      "source": [
        "## Exporting the model\n",
        "Next step is to export our model.\n",
        "We will export in `tf.SavedModel`, so it is possible to reload the model on TF.js"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdjMwXSIQLGs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}